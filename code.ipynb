{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhnunSxKHhUo",
        "outputId": "b89dc0a5-5d34-4fa6-b102-3a93e0778529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (3.6.0)\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.2.0.tar.gz (2.6 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'error'\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  √ó Getting requirements to build wheel did not run successfully.\n",
            "  ‚îÇ exit code: 1\n",
            "  ‚ï∞‚îÄ> [48 lines of output]\n",
            "      Traceback (most recent call last):\n",
            "        File \u001b[35m\"c:\\Users\\user\\poetic2\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "          \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "        File \u001b[35m\"c:\\Users\\user\\poetic2\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
            "          json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
            "                                   \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "        File \u001b[35m\"c:\\Users\\user\\poetic2\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
            "          return hook(config_settings)\n",
            "        File \u001b[35m\"C:\\Users\\user\\AppData\\Local\\Temp\\pip-build-env-1giq82p0\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
            "          return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
            "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "        File \u001b[35m\"C:\\Users\\user\\AppData\\Local\\Temp\\pip-build-env-1giq82p0\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
            "          \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "          \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "        File \u001b[35m\"C:\\Users\\user\\AppData\\Local\\Temp\\pip-build-env-1giq82p0\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
            "          \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
            "          \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "        File \u001b[35m\"C:\\Users\\user\\AppData\\Local\\Temp\\pip-build-env-1giq82p0\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
            "          \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
            "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
            "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m128\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "        File \u001b[35m\"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m416\u001b[0m, in \u001b[35mcheck_call\u001b[0m\n",
            "          retcode = call(*popenargs, **kwargs)\n",
            "        File \u001b[35m\"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m397\u001b[0m, in \u001b[35mcall\u001b[0m\n",
            "          with \u001b[31mPopen\u001b[0m\u001b[1;31m(*popenargs, **kwargs)\u001b[0m as p:\n",
            "               \u001b[31m~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "        File \u001b[35m\"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m1038\u001b[0m, in \u001b[35m__init__\u001b[0m\n",
            "          \u001b[31mself._execute_child\u001b[0m\u001b[1;31m(args, executable, preexec_fn, close_fds,\u001b[0m\n",
            "          \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "                              \u001b[1;31mpass_fds, cwd, env,\u001b[0m\n",
            "                              \u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "          ...<5 lines>...\n",
            "                              \u001b[1;31mgid, gids, uid, umask,\u001b[0m\n",
            "                              \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "                              \u001b[1;31mstart_new_session, process_group)\u001b[0m\n",
            "                              \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "        File \u001b[35m\"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m1550\u001b[0m, in \u001b[35m_execute_child\u001b[0m\n",
            "          hp, ht, pid, tid = \u001b[31m_winapi.CreateProcess\u001b[0m\u001b[1;31m(executable, args,\u001b[0m\n",
            "                             \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "                                   \u001b[1;31m# no special security\u001b[0m\n",
            "                                   \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "          ...<4 lines>...\n",
            "                                   \u001b[1;31mcwd,\u001b[0m\n",
            "                                   \u001b[1;31m^^^^\u001b[0m\n",
            "                                   \u001b[1;31mstartupinfo)\u001b[0m\n",
            "                                   \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
            "      \u001b[1;35mFileNotFoundError\u001b[0m: \u001b[35m[WinError 2] The system cannot find the file specified\u001b[0m\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: subprocess-exited-with-error\n",
            "\n",
            "√ó Getting requirements to build wheel did not run successfully.\n",
            "‚îÇ exit code: 1\n",
            "‚ï∞‚îÄ> See above for output.\n",
            "\n",
            "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets sentencepiece accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfUCm-WoIFrk",
        "outputId": "0d3832f0-c883-46d6-93dc-4822dc786c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title:‡§™‡§æ‡§™ ‡§≤‡§æ‡§ó‡•ç‡§õ\n",
            "Author:‡§≤‡§ï‡•ç‡§∑‡•ç‡§Æ‡•Ä‡§™‡•ç‡§∞‡§∏‡§æ‡§¶ ‡§¶‡•á‡§µ‡§ï‡•ã‡§ü‡§æ\n",
            "‡§®‡§ü‡§ø‡§™‡•ç‡§®‡•Å ‡§π‡•á‡§∞ ‡§ï‡•ã‡§™‡§ø‡§≤‡§æ!\n",
            "‡§®‡§ö‡•Å‡§Å‡§°‡•ç‡§®‡•Å ‡§™‡§æ‡§™ ‡§≤‡§æ‡§ó‡•ç‡§¶‡§õ‡•§\n",
            "‡§®‡§ö‡•ç‡§Ø‡§æ‡§§‡•ç‡§®‡•Å ‡§´‡•Ç‡§≤ ‡§®‡§æ‡§®‡§ø ‡§π‡•ã!\n",
            "‡§¶‡§Ø‡§æ ‡§∞ ‡§ß‡§∞‡•ç‡§Æ ‡§≠‡§æ‡§ó‡•ç‡§¶‡§õ‡•§‡•§\n",
            "\n",
            "‡§®‡§õ‡•ã‡§™‡•ç‡§®‡•Å ‡§π‡•à ‡§ö‡§∞‡•Ä ‡§¨‡§∞‡•Ä\n",
            "‡§∏‡§∞‡§æ‡§™ ‡§Ü‡§Å‡§∏‡•Å ‡§≤‡§æ‡§ó‡•ç‡§¶‡§õ‡•§\n",
            "‡§®‡§Æ‡§æ‡§∞‡•ç‡§®‡•Å ‡§ú‡§®‡•ç‡§§‡•Å ‡§π‡•à ‡§ï‡•Å‡§®‡•à\n",
            "‡§¨‡§∏‡•á‡§∞ ‡§ï‡§æ‡§≤ ‡§ú‡§æ‡§ó‡•ç‡§¶‡§õ‡•§‡•§\n",
            "\n",
            "‡§® ‡§ò‡§æ‡§â ‡§ö‡•ã‡§ü ‡§≤‡§æ‡§â‡§®‡•Ç\n",
            "‡§∏‡§°‡•á‡§∞ ‡§ö‡§ø‡§§‡•ç‡§§ ‡§™‡§æ‡§ï‡•ç‡§¶‡§õ‡•§\n",
            "‡§ß‡•Ç‡§≤‡•ã ‡§®‡§´‡•á‡§ï‡•ç‡§®‡•Å ‡§®‡§æ‡§®‡§ø ‡§π‡•ã!\n",
            "‡§â‡§°‡•á‡§∞ ‡§≠‡§ø‡§§‡•ç‡§∞ ‡§¢‡§æ‡§ï‡•ç‡§¶‡§õ‡•§‡•§\n",
            "\n",
            "‡§π‡§ø‡§≤‡•ã ‡§®‡§ñ‡•á‡§≤‡•ç‡§®‡•Å ‡§´‡•ã‡§π‡§∞‡•Ä\n",
            "‡§ñ‡§∞‡§æ‡§¨ ‡§¶‡§æ‡§ó ‡§≤‡§æ‡§ó‡•ç‡§¶‡§õ‡•§\n",
            "‡§® ‡§ö‡§ø‡§§‡•ç‡§§ ‡§π‡•à ‡§¶‡•Å‡§ñ‡§æ‡§â‡§®‡•Å\n",
            "‡§°‡§∏‡•á‡§∞ ‡§Ü‡§Å‡§∏‡•Å ‡§¨‡§ó‡•ç‡§¶‡§õ‡•§‡•§\n",
            "\n",
            "‡§¨‡§®‡•á‡§∞ ‡§´‡•Ç‡§≤ ‡§ù‡•à‡§Ç ‡§∏‡§ß‡•à‡§Ç\n",
            "‡§π‡§Å‡§∏‡§æ‡§â‡§®‡•Ç ‡§∏‡•Å‡§µ‡§æ‡§∏ ‡§¶‡•Ä‡•§\n",
            "‡§∏‡§ß‡•à‡§Ç ‡§∞‡§Æ‡§æ‡§â‡§®‡•Ç ‡§ú‡§ó‡§§‡•ç\n",
            "‡§∞‡§Æ‡•á‡§∞ ‡§®‡§ø‡§§‡•ç‡§Ø ‡§Ü‡§∂ ‡§¶‡•Ä‡•§‡•§\n",
            "\n",
            "‡§ú‡§§‡§æ‡§§‡§§‡•à ‡§õ ‡§à‡§∂ ‡§∞‡•á\n",
            "‡§õ ‡§∏‡•Å‡§®‡•ç‡§®‡•Å ‡§§‡•ç‡§Ø‡•ã ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§∞‡•á‡•§\n",
            "‡§õ‡§ï‡§æ‡§â‡§®‡•á ‡§≤‡•Å‡§ï‡§æ‡§â‡§®‡•á\n",
            "‡§®‡§∞‡§æ‡§ñ ‡§≠‡§æ‡§µ ‡§ï‡•ç‡§Ø‡•à ‡§ó‡§∞‡•á!\n"
          ]
        }
      ],
      "source": [
        "# Load and parse Nepali poem dataset\n",
        "def load_poems(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Split poems using two or more newlines\n",
        "    poems = [p.strip() for p in content.strip().split('\\n\\n\\n') if p.strip()]\n",
        "    return poems\n",
        "\n",
        "# Extract and print the first poem\n",
        "def print_first_poem(file_path):\n",
        "    poems = load_poems(file_path)\n",
        "    if poems:\n",
        "        print(poems[0])\n",
        "    else:\n",
        "        print(\"No poems found.\")\n",
        "\n",
        "# Example usage\n",
        "print_first_poem('allpoems.txt')  # Replace with your dataset filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTgsT6OGIFuC",
        "outputId": "869fded4-7861-4ce8-daeb-023f87180fad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\poetic2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "#Loading the model and tokenizer\n",
        "model_name = \"Sakonii/distilgpt2-nepali\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5MyudgKIFxB",
        "outputId": "4d2a3e5c-820c-4ff8-b238-d66f83ccf2c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from datasets) (2.2.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, max_length=512)\n",
        "\n",
        "# Load the dataset using the 'datasets' library\n",
        "dataset = load_dataset('text', data_files='allpoems.txt')\n",
        "\n",
        "# Now you can tokenize the dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpNXneFyIF1a",
        "outputId": "70ca98a2-8340-4862-8e63-c9597ad96849"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\poetic2\\.venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextDataset\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size=128):\n",
        "    return TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size\n",
        "    )\n",
        "\n",
        "train_dataset = load_dataset(\"allpoems.txt\", tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_psNgryPIF4B"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fkMtLucJIF6I"
      },
      "outputs": [],
      "source": [
        "# you need to set parameters\n",
        "train_file_path = \"allpoems.txt\"\n",
        "model_name = 'Sakonii/distilgpt2-nepali'\n",
        "output_dir=\"./distilgpt2-nepali-poem\",\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 250\n",
        "save_steps = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZoDvkxtNIF95"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer # Importing TrainingArguments and Trainer\n",
        "\n",
        "def train(train_file_path,model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "  ### Loads model for finetuning\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = TrainingArguments( # TrainingArguments was not imported hence the error\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = Trainer( # Trainer was also not imported and needs to be imported\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpkOuU7Zf3u-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcNxzUTxf32t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PY_V7svyf37V"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (1.6.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from accelerate) (2.2.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from accelerate) (2.7.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from accelerate) (0.31.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (80.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\poetic2\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "1JPwzjgMIGAg",
        "outputId": "98380c36-d0b7-454b-e8c2-7f5343d44241"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37500/37500 2:36:37, Epoch 250/250]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>4.826200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.227700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.009900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.144300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.640800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.378900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.249600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.193200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.163100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.141500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.127000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.118200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.108900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.103600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.098800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.093700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.090700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.087000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.084400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.082800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.079900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.078500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.076800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.075500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.073600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.072200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.071200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.070800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.069500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.068900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.067300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.067400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.066900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.066700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.066200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.065500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.064700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.064500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.063600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.063200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.062900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.062500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.062300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.062900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.062100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.061200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.061400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.061200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.060600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.060600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.060400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.060000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.059700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.059700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.059300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.059400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.059100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.059000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.058900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.058800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.058500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.058300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.058300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.057900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.057700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.057700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.057600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.057500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>0.057300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.057100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>0.057000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.057000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>0.056800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.056700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>0.056500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train(\n",
        "    train_file_path = \"allpoems.txt\",\n",
        "    model_name = 'Sakonii/distilgpt2-nepali',\n",
        "    output_dir=\"./distilgpt2-nepali-poem\",\n",
        "    overwrite_output_dir = False,\n",
        "    per_device_train_batch_size = 8,\n",
        "    num_train_epochs = 250,\n",
        "    save_steps = 500\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9JFFIKZIGDA"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.rmtree('distilgpt2-nepali-poem/checkpoint-1000')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K8YTPJcWIGFV"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def generate_poem(sequence, max_length):\n",
        "    model_path = \"C:/Users/user/poetic2/distilgpt2-nepali-poem\"\n",
        "    model = load_model(model_path)\n",
        "    tokenizer = load_tokenizer(model_path)\n",
        "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1HF2kfF8IGK5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‡§Æ ‡§∞ ‡§≠‡§∞‡§ø‡§Ø‡§æ ‡§ò‡§°‡•Ä ‡§∏‡§æ‡§π ‡§∞‡§æ‡§Æ‡•ç‡§∞‡§æ ‡§∏‡•Å‡§ò‡§∞ ‡§∏‡•Å‡§∞ ‡§ú‡§∏‡•ç‡§§‡§æ ‡§∂‡§π‡§∞‡§ø‡§Ø‡§æ ‡•§ ‡§ò‡§°‡•Ä ‡§ò‡§æ‡§Å‡§∏‡•Ä ‡§ó‡•ç‡§µ‡§æ‡§≤‡§æ, ‡§ï‡•É‡§∑‡§ï‡§π‡§∞‡•Å ‡§ù‡•Å‡§§‡•ç‡§∞‡•à ‡§®‡§ó‡§ø‡§ö‡§Æ‡§æ ‡§•‡§ø‡§Ø‡•á‡§Å ‡§∏‡§æ‡§ï‡•ç‡§∑‡•Ä ‡§ú‡§∏‡•ç‡§§‡•ã ‡§∏‡§ï‡§≤ ‡§¨‡§ü‡•Å‡§µ‡§æ‡§ï‡•ã ‡§Æ ‡§¨‡§ø‡§ö‡§Æ‡§æ ‡•• ‡§≠‡§ø‡§ï‡•Ä ‡§∏‡•Å‡§∏‡•ç‡§ï‡•á‡§∞‡§æ‡§ï‡•ã ‡§¨‡§π‡•Å‡§§ ‡§ñ‡§ø‡§∞‡§ø‡§≤‡•ã ‡§∏‡•Ç‡§§ ‡§â‡§∏‡§Æ‡§æ ‡§â‡§®‡•Ä ‡§Æ‡§æ‡§≤‡§æ ‡§Æ‡•ã‡§§‡•Ä‡§∏‡§¶‡•É‡§∂ ‡§™‡§∏‡§ø‡§®‡§æ‡§ï‡•ã ‡§¶‡§ø‡§µ‡§∏‡§Æ‡§æ ‡•§ ‡§Æ‡§≤‡§æ‡§à ‡§¶‡§ø‡§®‡•ç‡§•‡•á ‡§ú‡•ã ‡§™‡§•‡§ø‡§ï‡§π‡§∞‡•Å ‡§§‡•Ä ‡§¨‡•Ä‡§ö ‡§™‡§•‡§Æ‡§æ ‡§∏‡§¨‡•à ‡§∂‡•ã‡§ö‡•Ä (‡§ß‡•Ä) ‡§≤‡§ø‡§®‡•ç‡§•‡•á‡§Å ‡§Æ ‡§™‡§®‡§ø ‡§ó‡§π‡§®‡§æ ‡§§‡•ç‡§Ø‡•ã ‡§∏‡•Å‡§™‡§•‡§Æ‡§æ ‡•• ‡§ú‡§§‡•Ä ‡§™‡•ç‡§Ø‡•Å\n"
          ]
        }
      ],
      "source": [
        "sequence = input()\n",
        "max_len = int(input())\n",
        "generate_poem(sequence, max_len)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
